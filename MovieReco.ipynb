{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Movie Recommendation System "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Citation \n",
    "# F. Maxwell Harper and Joseph A. Konstan. 2015. The MovieLens Datasets: History and Context. ACM Transactions on Interactive Intelligent Systems (TiiS) 5, 4, Article 19 (December 2015), 19 pages. DOI=http://dx.doi.org/10.1145/2827872"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the required packages and load data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Click here to create your ratings DataFrame\n",
    "import ibmos2spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('Number of partitions for the movies DataFrame: {}'.format(movies.rdd.getNumPartitions()))\n",
    "print('Number of partitions for the ratings DataFrame: {}'.format(ratings.rdd.getNumPartitions()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Create repartitioned ratings data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of ratings: {}'.format(repartitionedRatings.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repartitionedRatings.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of ratings: {}'.format(repartitionedRatings.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paste here your project token.It will look like this\n",
    "from project_lib import Project\n",
    "project = Project(sc, '******************', '******************')\n",
    "pc = project.project_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_file_name = 'movies.csv'\n",
    "ratings_file_name = 'ratings.csv'\n",
    "\n",
    "movies = spark.read.csv(project.get_file_url(movies_file_name), header=True, inferSchema=True).repartition(10).cache()\n",
    "ratings = spark.read.csv(project.get_file_url(ratings_file_name), header=True, inferSchema=True).repartition(10).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies.printSchema()\n",
    "ratings.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of different users: {}'.format(ratings.select('userId').distinct().count()))\n",
    "print('Number of different movies: {}'.format(ratings.select('movieId').distinct().count()))\n",
    "print('Number of movies with at least one rating strictly higher than 4: {}'.format(ratings.filter('rating > 4').select('movieId').distinct().count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings.createOrReplaceTempView('ratings')\n",
    "spark.sql('SELECT COUNT(DISTINCT(movieId)) AS nb FROM ratings WHERE rating > 4').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_url = project.get_file_url(ratings_file_name)\n",
    "sql = 'SELECT * FROM csv.`' + ratings_url + '`'\n",
    "spark.sql(sql).take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "ratings.toPandas().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# there are many different ways to visualize the data \n",
    "# as this is not the focus of this project, only one method is shown \n",
    "\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "ratingsPandas = ratings.toPandas()\n",
    "sns.lmplot(x='userId', y='movieId', data=ratingsPandas, fit_reg=False);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the Recommender System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql('''\n",
    "    SELECT *, 100 * nb_ratings/matrix_size AS percentage\n",
    "    FROM (\n",
    "        SELECT nb_users, nb_movies, nb_ratings, nb_users * nb_movies AS matrix_size\n",
    "        FROM (\n",
    "            SELECT COUNT(*) AS nb_ratings, COUNT(DISTINCT(movieId)) AS nb_movies, COUNT(DISTINCT(userId)) AS nb_users\n",
    "            FROM ratings\n",
    "        )\n",
    "    )\n",
    "''').toPandas().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "from pyspark.ml.recommendation import ALS\n",
    "\n",
    "model = ALS(userCol='userId', itemCol='movieId', ratingCol='rating').fit(ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the Model \n",
    "predictions = model.transform(ratings)\n",
    "predictions.toPandas().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model \n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "evaluator = RegressionEvaluator(metricName='rmse', labelCol='rating', predictionCol='prediction')\n",
    "print('The root mean squared error for our model is: {}'.format(evaluator.evaluate(predictions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the Dataset into train and test\n",
    "(trainingRatings, testRatings) = ratings.randomSplit([80.0, 20.0])\n",
    "als = ALS(userCol='userId', itemCol='movieId', ratingCol='rating')\n",
    "model = als.fit(trainingRatings)\n",
    "predictions = model.transform(testRatings)\n",
    "\n",
    "predictions.toPandas().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overcoming NaN results with average rating \n",
    "avgRatings = ratings.select('rating').groupBy().avg().first()[0]\n",
    "print ('The average rating in the dataset is: {}'.format(avgRatings))\n",
    "\n",
    "evaluator = RegressionEvaluator(metricName='rmse', labelCol='rating', predictionCol='prediction')\n",
    "print ('The root mean squared error for our model is: {}'.format(evaluator.evaluate(predictions.na.fill(avgRatings))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exclude NaN\n",
    "evaluator = RegressionEvaluator(metricName='rmse', labelCol='rating', predictionCol='prediction')\n",
    "print ('The root mean squared error for our model is: {}'.format(evaluator.evaluate(predictions.na.drop())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def repeatALS(data, k=3, userCol='userId', itemCol='movieId', ratingCol='rating', metricName='rmse'):\n",
    "    evaluations = []\n",
    "    for i in range(0, k):  \n",
    "        (trainingSet, testingSet) = data.randomSplit([k - 1.0, 1.0])\n",
    "        als = ALS(userCol=userCol, itemCol=itemCol, ratingCol=ratingCol)\n",
    "        model = als.fit(trainingSet)\n",
    "        predictions = model.transform(testingSet)\n",
    "        evaluator = RegressionEvaluator(metricName=metricName, labelCol='rating', predictionCol='prediction')\n",
    "        evaluation = evaluator.evaluate(predictions.na.drop())\n",
    "        print('Loop {}: {} = {}'.format(i + 1, metricName, evaluation))\n",
    "        evaluations.append(evaluation)\n",
    "    return sum(evaluations) / float(len(evaluations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print('RMSE = {}'.format(repeatALS(ratings, k=4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def kfoldALS(data, k=3, userCol='userId', itemCol='movieId', ratingCol='rating', metricName='rmse'):\n",
    "    evaluations = []\n",
    "    weights = [1.0] * k\n",
    "    splits = data.randomSplit(weights)\n",
    "    for i in range(0, k):  \n",
    "        testingSet = splits[i]\n",
    "        trainingSet = spark.createDataFrame(sc.emptyRDD(), data.schema)\n",
    "        for j in range(0, k):\n",
    "            if i == j:\n",
    "                continue\n",
    "            else:\n",
    "                trainingSet = trainingSet.union(splits[j])\n",
    "        als = ALS(userCol=userCol, itemCol=itemCol, ratingCol=ratingCol)\n",
    "        model = als.fit(trainingSet)\n",
    "        predictions = model.transform(testingSet)\n",
    "        evaluator = RegressionEvaluator(metricName=metricName, labelCol='rating', predictionCol='prediction')\n",
    "        evaluation = evaluator.evaluate(predictions.na.drop())\n",
    "        print('Loop {}: {} = {}'.format(i + 1, metricName, evaluation))\n",
    "        evaluations.append(evaluation)\n",
    "    return sum(evaluations) / float(len(evaluations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "\n",
    "def recommendMovies(model, user, nbRecommendations):\n",
    "    # Create a Spark DataFrame with the specified user and all the movies listed in the ratings DataFrame\n",
    "    dataSet = appendedRatings.select('movieId').distinct().withColumn('userId', lit(user))\n",
    "\n",
    "    # Create a Spark DataFrame with the movies that have already been rated by this user\n",
    "    moviesAlreadyRated =  appendedRatings.filter(appendedRatings.userId == user).select('movieId', 'userId')\n",
    "\n",
    "    # Apply the recommender system to the data set without the already rated movies to predict ratings\n",
    "    predictions = model.transform(dataSet.subtract(moviesAlreadyRated)).dropna().orderBy('prediction', ascending=False).limit(nbRecommendations).select('movieId', 'prediction')\n",
    "\n",
    "    # Join with the movies DataFrame to get the movies titles and genres\n",
    "    recommendations = predictions.join( movies, predictions.movieId ==  movies.movieId).select(predictions.movieId,  movies.title,  movies.genres, predictions.prediction)\n",
    "\n",
    "#     recommendations.show(truncate=False)\n",
    "    return dataSet, moviesAlreadyRated, predictions, recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test prediction\n",
    "appendedRatings = ratings\n",
    "\n",
    "print('Recommendations for user 133:')\n",
    "recommendMovies(model, 133, 10).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a New User"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newUserID = int(ratingsPandas[['userId']].max()) +1\n",
    "moviesPandas.sample(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_array = list()\n",
    "number = 10\n",
    "print ('Enter numbers in array: ')\n",
    "for i in range(number):\n",
    "    n = input(\"MovieID :\")\n",
    "    number_array.append(int(n))\n",
    "print ('Your Selected Movie IDs: ',number_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newUserID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['userId','movieId','rating','timestamp']\n",
    "\n",
    "appendedRatings = ratings\n",
    "from pyspark.sql.types import IntegerType\n",
    "import random\n",
    "for i in range(number):\n",
    "    newRow = spark.createDataFrame([(newUserID,number_array[i],float(random.randint(1,5)),int(0))], schema=columns)\n",
    "    df2 = newRow.withColumn(\"userId\", newRow[\"userId\"].cast(IntegerType()))\n",
    "    df3 = df2.withColumn(\"movieId\", newRow[\"movieId\"].cast(IntegerType()))\n",
    "    df4 = df3.withColumn(\"timestamp\", newRow[\"timestamp\"].cast(IntegerType()))\n",
    "\n",
    "    appendedRatings = appendedRatings.union(df4)\n",
    "\n",
    "appendedRatings.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "appendedRatings.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a,b,c,d = recommendMovies(model, 610, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b.select('movieId').distinct().toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "d.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w,x,y,z = recommendMovies(model, 611, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "w.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings.printSchema()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}